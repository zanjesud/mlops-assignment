name: MLOps CI/CD Pipeline

on:
  push:
    branches: [master, develop]
    paths:
      - 'data/raw/iris.csv.dvc'
      - 'api/**'
      - 'ui/**'
      - 'models/**'
      - 'scripts/**'
      - 'tests/**'
      - 'pyproject.toml'
      - '.github/workflows/**'
  pull_request:
    branches: [master]
  workflow_dispatch:
    inputs:
      force_promotion:
        description: 'Force promotion to production'
        required: false
        default: false
        type: boolean
      trigger_training:
        description: 'Force model training'
        required: false
        default: false
        type: boolean

permissions:
  contents: write
  pull-requests: write
  packages: write
  actions: read

env:
  DOCKER_HUB_USERNAME: ${{ secrets.DOCKER_HUB_USERNAME }}
  DOCKER_HUB_TOKEN: ${{ secrets.DOCKER_HUB_TOKEN }}

jobs:
  # Stage 1: Code Quality & Testing
  quality-check:
    runs-on: ubuntu-latest
    name: "Code Quality & Testing"
    strategy:
      matrix:
        python-version: ["3.11"]
      fail-fast: false
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
      
      - name: Install uv
        run: pip install uv
      
      - name: Create virtual environment
        run: uv venv .venv
      
      - name: Install dependencies
        run: uv pip install -e .
      
      - name: Setup DVC and pull data for tests
        env:
          GDRIVE_SERVICE_ACCOUNT_JSON: ${{ secrets.GDRIVE_SERVICE_ACCOUNT_JSON }}
        run: |
          if [ -f "data/raw/iris.csv.dvc" ] && [ ! -f "data/raw/iris.csv" ]; then
            echo "Setting up DVC with service account..."
            echo "$GDRIVE_SERVICE_ACCOUNT_JSON" > /tmp/gdrive_service_account.json
            uv run dvc remote modify gdrive gdrive_use_service_account true
            uv run dvc remote modify gdrive gdrive_service_account_json_file_path /tmp/gdrive_service_account.json
            uv run dvc pull data/raw/iris.csv.dvc || echo "DVC pull failed, continuing with existing data"
            rm -f /tmp/gdrive_service_account.json
          fi
      
      - name: Create necessary directories
        run: mkdir -p logs artifacts data/processed models/production_model
      
      - name: Lint with ruff
        run: uv run ruff check . --select E,F,W
      
      - name: Check code format
        run: |
          uv run black --check .
          uv run isort --check-only .
          uv run mypy .
      
      - name: Run unit tests
        run: uv run pytest tests/test_api.py -v --cov=api --cov-report=xml --cov-report=html
      
      - name: Run model evaluation tests
        run: |
          if [ -f "tests/test_model_evaluation.py" ]; then
            uv run pytest tests/test_model_evaluation.py -v --cov=models --cov-append --cov-report=xml --cov-report=html
          else
            echo "Model evaluation tests not found - skipping"
          fi
      
      - name: Upload coverage reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: coverage-reports-${{ matrix.python-version }}
          path: |
            coverage.xml
            htmlcov/
          retention-days: 30

  # Stage 2: Data-Driven Model Training
  model-training:
    runs-on: ubuntu-latest
    name: "Model Training & Validation"
    needs: quality-check
    if: needs.quality-check.result == 'success'
    
    outputs:
      model_trained: ${{ steps.training.outputs.model_trained }}
      run_id: ${{ steps.training.outputs.run_id }}
      model_updated: ${{ steps.training.outputs.model_updated }}
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          pip install uv
          uv venv .venv
          uv pip install -e .
      
      - name: Create directories
        run: mkdir -p logs artifacts data/processed models/production_model
      
      - name: Check if data changed
        id: data_check
        run: |
          DATA_CHANGED=false
          
          # Manual workflow trigger
          if [ "${{ github.event_name }}" = "workflow_dispatch" ] && [ "${{ github.event.inputs.trigger_training }}" = "true" ]; then
            echo "✅ Manual training trigger"
            DATA_CHANGED=true
          
          # For pushes and PRs, check if DVC file exists (workflow triggered by paths filter)
          elif [ "${{ github.event_name }}" = "push" ] || [ "${{ github.event_name }}" = "pull_request" ]; then
            if [ -f "data/raw/iris.csv.dvc" ]; then
              echo "✅ Workflow triggered and DVC file exists - training model"
              DATA_CHANGED=true
            else
              echo "❌ DVC file not found"
            fi
          fi
          
          if [ "$DATA_CHANGED" = "false" ]; then
            echo "❌ No data changes detected - skipping model training"
          fi
          
          echo "data_changed=$DATA_CHANGED" >> $GITHUB_OUTPUT
      
      - name: Setup DVC and pull updated data
        if: steps.data_check.outputs.data_changed == 'true'
        env:
          GDRIVE_SERVICE_ACCOUNT_JSON: ${{ secrets.GDRIVE_SERVICE_ACCOUNT_JSON }}
        run: |
          echo "Setting up DVC with service account authentication..."
          echo "$GDRIVE_SERVICE_ACCOUNT_JSON" > /tmp/gdrive_service_account.json
          
          # Configure DVC to use service account
          uv run dvc remote modify gdrive gdrive_use_service_account true
          uv run dvc remote modify gdrive gdrive_service_account_json_file_path /tmp/gdrive_service_account.json
          
          # Pull updated data
          echo "Pulling updated data with DVC..."
          uv run dvc pull data/raw/iris.csv.dvc
          
          # Clean up service account file
          rm -f /tmp/gdrive_service_account.json
          
          # Validate data exists
          if [ ! -f "data/raw/iris.csv" ]; then
            echo "Error: iris.csv not found after DVC pull"
            exit 1
          fi
          
          echo "Data validation passed: $(wc -l < data/raw/iris.csv) lines"
      
      - name: Train and validate model
        id: training
        if: steps.data_check.outputs.data_changed == 'true'
        run: |
          echo "Starting model training on updated data..."
          
          # Train model and capture run ID
          RUN_ID=$(uv run python scripts/train_on_data_update.py 2>&1 | tail -n 1 | grep -E '^[a-f0-9]{32}$')
          
          if [ ! -z "$RUN_ID" ]; then
            echo "model_trained=true" >> $GITHUB_OUTPUT
            echo "run_id=$RUN_ID" >> $GITHUB_OUTPUT
            echo "Model training successful. Run ID: $RUN_ID"
            
            # Promote to staging
            if uv run python scripts/promote_to_staging.py --run_id $RUN_ID; then
              echo "Model promoted to staging"
              
              # If on master, promote to production
              if [ "${{ github.ref }}" = "refs/heads/master" ] || [ "${{ github.event.inputs.force_promotion }}" = "true" ]; then
                if uv run python scripts/promote_to_production.py; then
                  echo "model_updated=true" >> $GITHUB_OUTPUT
                  echo "Model promoted to production and workspace updated"
                else
                  echo "model_updated=false" >> $GITHUB_OUTPUT
                fi
              else
                echo "model_updated=false" >> $GITHUB_OUTPUT
                echo "Skipping production promotion (not on master)"
              fi
            else
              echo "model_updated=false" >> $GITHUB_OUTPUT
              exit 1
            fi
          else
            echo "model_trained=false" >> $GITHUB_OUTPUT
            echo "model_updated=false" >> $GITHUB_OUTPUT
            exit 1
          fi
      
      - name: Upload model artifacts
        if: steps.data_check.outputs.data_changed == 'true' && steps.training.outputs.model_trained == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: model-artifacts
          path: |
            artifacts/
            logs/
            models/production_model/
          retention-days: 30

  # Stage 3: Build & Deploy
  build-and-deploy:
    runs-on: ubuntu-latest
    name: "Build & Deploy"
    needs: [quality-check, model-training]
    if: always() && needs.quality-check.result == 'success'
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Python for DVC
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install DVC and pull data for Docker build
        env:
          GDRIVE_SERVICE_ACCOUNT_JSON: ${{ secrets.GDRIVE_SERVICE_ACCOUNT_JSON }}
        run: |
          pip install uv
          uv venv .venv
          uv pip install dvc[gdrive]
          
          # Pull data if DVC file exists
          if [ -f "data/raw/iris.csv.dvc" ]; then
            echo "Setting up DVC for Docker build..."
            echo "$GDRIVE_SERVICE_ACCOUNT_JSON" > /tmp/gdrive_service_account.json
            uv run dvc remote modify gdrive gdrive_use_service_account true
            uv run dvc remote modify gdrive gdrive_service_account_json_file_path /tmp/gdrive_service_account.json
            uv run dvc pull data/raw/iris.csv.dvc || echo "DVC pull failed, using cached data"
            rm -f /tmp/gdrive_service_account.json
          fi
      
      - name: Download model artifacts (if available)
        if: needs.model-training.outputs.model_trained == 'true'
        uses: actions/download-artifact@v4
        with:
          name: model-artifacts
          path: .
      
      - name: Commit updated production model
        if: needs.model-training.outputs.model_updated == 'true'
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          
          git add models/production_model/
          if ! git diff --staged --quiet; then
            git commit -m "Update production model from CI/CD [skip ci]"
            git push
          fi
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      
      - name: Log in to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ env.DOCKER_HUB_USERNAME }}
          password: ${{ env.DOCKER_HUB_TOKEN }}
      
      - name: Determine image tags
        id: tags
        run: |
          if [ "${{ github.event_name }}" = "pull_request" ]; then
            echo "IMAGE_TAG=pr-${{ github.event.number }}" >> $GITHUB_OUTPUT
          elif [ "${{ github.ref }}" = "refs/heads/master" ]; then
            echo "IMAGE_TAG=latest" >> $GITHUB_OUTPUT
          else
            echo "IMAGE_TAG=dev-${{ github.sha }}" >> $GITHUB_OUTPUT
          fi
          
          # Add model update suffix if model was updated
          if [ "${{ needs.model-training.outputs.model_updated }}" = "true" ]; then
            echo "MODEL_SUFFIX=-model-$(date +%Y%m%d-%H%M%S)" >> $GITHUB_OUTPUT
          else
            echo "MODEL_SUFFIX=" >> $GITHUB_OUTPUT
          fi
      
      - name: Build and push Docker images
        run: |
          # Build images with latest tag first
          export DOCKER_HUB_USERNAME=${{ env.DOCKER_HUB_USERNAME }}
          export IMAGE_TAG=latest
          docker compose build
          
          # Create additional tags
          FINAL_TAG=${{ steps.tags.outputs.IMAGE_TAG }}${{ steps.tags.outputs.MODEL_SUFFIX }}
          
          # Tag images with final tag
          docker tag $DOCKER_HUB_USERNAME/mlops-api:latest $DOCKER_HUB_USERNAME/mlops-api:$FINAL_TAG
          docker tag $DOCKER_HUB_USERNAME/mlops-ui:latest $DOCKER_HUB_USERNAME/mlops-ui:$FINAL_TAG
          docker tag $DOCKER_HUB_USERNAME/mlops-mlflow:latest $DOCKER_HUB_USERNAME/mlops-mlflow:$FINAL_TAG
          
          # Push images
          docker push $DOCKER_HUB_USERNAME/mlops-api:latest
          docker push $DOCKER_HUB_USERNAME/mlops-api:$FINAL_TAG
          docker push $DOCKER_HUB_USERNAME/mlops-ui:latest
          docker push $DOCKER_HUB_USERNAME/mlops-ui:$FINAL_TAG
          docker push $DOCKER_HUB_USERNAME/mlops-mlflow:latest
          docker push $DOCKER_HUB_USERNAME/mlops-mlflow:$FINAL_TAG
      
      - name: Test Docker images
        run: |
          echo "Testing API image..."
          docker run --rm -d --name test-api -p 8000:8000 ${{ env.DOCKER_HUB_USERNAME }}/mlops-api:latest
          
          sleep 15
          
          if curl -f http://localhost:8000/health; then
            echo "✅ API health check passed!"
          else
            echo "❌ API health check failed"
            docker logs test-api
            docker stop test-api
            exit 1
          fi
          
          docker stop test-api

  # Stage 4: Pipeline Summary
  notify:
    runs-on: ubuntu-latest
    name: "Pipeline Summary"
    needs: [quality-check, model-training, build-and-deploy]
    if: always()
    
    steps:
      - name: Pipeline Summary
        run: |
          echo "=== MLOps CI/CD Pipeline Summary ==="
          echo "Quality Check: ${{ needs.quality-check.result }}"
          echo "Model Training: ${{ needs.model-training.result }}"
          echo "Build & Deploy: ${{ needs.build-and-deploy.result }}"
          
          if [ "${{ needs.build-and-deploy.result }}" = "success" ]; then
            if [ "${{ needs.model-training.outputs.model_updated }}" = "true" ]; then
              echo "🎉 Full pipeline with model update completed!"
              echo "✅ Code quality checks passed"
              echo "✅ Model retrained on updated data"
              echo "✅ Model promoted to production"
              echo "✅ Docker images built with updated model"
            else
              echo "✅ Standard pipeline completed successfully!"
              echo "✅ Code quality checks passed"
              echo "✅ Docker images built and deployed"
            fi
          else
            echo "❌ Pipeline failed - check logs above"
          fi