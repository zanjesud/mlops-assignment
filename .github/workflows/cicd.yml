name: MLOps CI/CD Pipeline

on:
  push:
    branches: [master, develop]
    paths:
      - 'data/raw/iris.csv.dvc'
      - 'api/**'
      - 'ui/**'
      - 'models/**'
      - 'scripts/**'
      - 'tests/**'
      - 'pyproject.toml'
      - '.github/workflows/**'
  pull_request:
    branches: [master]
  workflow_dispatch:
    inputs:
      force_promotion:
        description: 'Force promotion to production'
        required: false
        default: false
        type: boolean
      trigger_training:
        description: 'Force model training'
        required: false
        default: false
        type: boolean

permissions:
  contents: write
  pull-requests: write
  packages: write
  actions: read

env:
  DOCKER_HUB_USERNAME: ${{ secrets.DOCKER_HUB_USERNAME }}
  DOCKER_HUB_TOKEN: ${{ secrets.DOCKER_HUB_TOKEN }}

jobs:
  # Stage 1: Code Quality & Testing
  quality-check:
    runs-on: ubuntu-latest
    name: "Code Quality & Testing"
    strategy:
      matrix:
        python-version: ["3.11"]
      fail-fast: false
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
      
      - name: Install uv
        run: pip install uv
      
      - name: Create virtual environment
        run: uv venv .venv
      
      - name: Install dependencies
        run: uv pip install -e .
      
      - name: Setup DVC and pull data for tests
        env:
          GDRIVE_SERVICE_ACCOUNT_JSON: ${{ secrets.GDRIVE_SERVICE_ACCOUNT_JSON }}
        run: |
          if [ -f "data/raw/iris.csv.dvc" ] && [ ! -f "data/raw/iris.csv" ]; then
            echo "Setting up DVC with service account..."
            echo "$GDRIVE_SERVICE_ACCOUNT_JSON" > /tmp/gdrive_service_account.json
            uv run dvc remote modify gdrive gdrive_use_service_account true
            uv run dvc remote modify gdrive gdrive_service_account_json_file_path /tmp/gdrive_service_account.json
            uv run dvc pull data/raw/iris.csv.dvc || echo "DVC pull failed, continuing with existing data"
            rm -f /tmp/gdrive_service_account.json
          fi
      
      - name: Create necessary directories
        run: mkdir -p logs artifacts data/processed models/production_model
      
      - name: Lint with ruff
        run: uv run ruff check . --select E,F,W
      
      - name: Check code format
        run: |
          uv run black --check .
          uv run isort --check-only .
          uv run mypy .
      
      - name: Run unit tests
        run: uv run pytest tests/test_api.py -v --cov=api --cov-report=xml --cov-report=html
      
      - name: Run model evaluation tests
        run: |
          if [ -f "tests/test_model_evaluation.py" ]; then
            uv run pytest tests/test_model_evaluation.py -v --cov=models --cov-append --cov-report=xml --cov-report=html
          else
            echo "Model evaluation tests not found - skipping"
          fi
      
      - name: Upload coverage reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: coverage-reports-${{ matrix.python-version }}
          path: |
            coverage.xml
            htmlcov/
          retention-days: 30

  # Stage 2: Data-Driven Model Training
  model-training:
    runs-on: ubuntu-latest
    name: "Model Training & Validation"
    needs: quality-check
    if: needs.quality-check.result == 'success'
    
    outputs:
      model_trained: ${{ steps.training.outputs.model_trained }}
      run_id: ${{ steps.training.outputs.run_id }}
      model_updated: ${{ steps.training.outputs.model_updated }}
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          pip install uv
          uv venv .venv
          uv pip install -e .
      
      - name: Create directories
        run: mkdir -p logs artifacts data/processed models/production_model
      
      - name: Check if data changed
        id: data_check
        run: |
          DATA_CHANGED=false
          
          # Manual workflow trigger
          if [ "${{ github.event_name }}" = "workflow_dispatch" ] && [ "${{ github.event.inputs.trigger_training }}" = "true" ]; then
            echo "‚úÖ Manual training trigger"
            DATA_CHANGED=true
          
          # For pushes and PRs, check if DVC file exists (workflow triggered by paths filter)
          elif [ "${{ github.event_name }}" = "push" ] || [ "${{ github.event_name }}" = "pull_request" ]; then
            if [ -f "data/raw/iris.csv.dvc" ]; then
              echo "‚úÖ Workflow triggered and DVC file exists - training model"
              DATA_CHANGED=true
            else
              echo "‚ùå DVC file not found"
            fi
          fi
          
          if [ "$DATA_CHANGED" = "false" ]; then
            echo "‚ùå No data changes detected - skipping model training"
          fi
          
          echo "data_changed=$DATA_CHANGED" >> $GITHUB_OUTPUT
      
      - name: Setup DVC and pull updated data
        if: steps.data_check.outputs.data_changed == 'true'
        env:
          GDRIVE_SERVICE_ACCOUNT_JSON: ${{ secrets.GDRIVE_SERVICE_ACCOUNT_JSON }}
        run: |
          echo "Setting up DVC with service account authentication..."
          echo "$GDRIVE_SERVICE_ACCOUNT_JSON" > /tmp/gdrive_service_account.json
          
          # Configure DVC to use service account
          uv run dvc remote modify gdrive gdrive_use_service_account true
          uv run dvc remote modify gdrive gdrive_service_account_json_file_path /tmp/gdrive_service_account.json
          
          # Pull updated data
          echo "Pulling updated data with DVC..."
          uv run dvc pull data/raw/iris.csv.dvc
          
          # Clean up service account file
          rm -f /tmp/gdrive_service_account.json
          
          # Validate data exists
          if [ ! -f "data/raw/iris.csv" ]; then
            echo "Error: iris.csv not found after DVC pull"
            exit 1
          fi
          
          echo "Data validation passed: $(wc -l < data/raw/iris.csv) lines"
      
      - name: Train and validate model
        id: training
        if: steps.data_check.outputs.data_changed == 'true'
        run: |
          echo "Starting model training on updated data..."
          
          # Train model and capture run ID
          RUN_ID=$(uv run python scripts/train_on_data_update.py 2>&1 | tail -n 1 | grep -E '^[a-f0-9]{32}$')
          
          if [ ! -z "$RUN_ID" ]; then
            echo "model_trained=true" >> $GITHUB_OUTPUT
            echo "run_id=$RUN_ID" >> $GITHUB_OUTPUT
            echo "Model training successful. Run ID: $RUN_ID"
            
            # Promote to staging
            if uv run python scripts/promote_to_staging.py --run_id $RUN_ID; then
              echo "Model promoted to staging"
              
              # If on master, promote to production
              if [ "${{ github.ref }}" = "refs/heads/master" ] || [ "${{ github.event.inputs.force_promotion }}" = "true" ]; then
                if uv run python scripts/promote_to_production.py; then
                  echo "model_updated=true" >> $GITHUB_OUTPUT
                  echo "Model promoted to production and workspace updated"
                else
                  echo "model_updated=false" >> $GITHUB_OUTPUT
                fi
              else
                echo "model_updated=false" >> $GITHUB_OUTPUT
                echo "Skipping production promotion (not on master)"
              fi
            else
              echo "model_updated=false" >> $GITHUB_OUTPUT
              exit 1
            fi
          else
            echo "model_trained=false" >> $GITHUB_OUTPUT
            echo "model_updated=false" >> $GITHUB_OUTPUT
            exit 1
          fi
      
      - name: Upload model artifacts
        if: steps.data_check.outputs.data_changed == 'true' && steps.training.outputs.model_trained == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: model-artifacts
          path: |
            artifacts/
            logs/
            models/production_model/
          retention-days: 30

  # Stage 3: Build & Deploy
  build-and-deploy:
    runs-on: ubuntu-latest
    name: "Build & Deploy"
    needs: [quality-check, model-training]
    if: always() && needs.quality-check.result == 'success'
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Python for DVC
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install DVC and pull data for Docker build
        env:
          GDRIVE_SERVICE_ACCOUNT_JSON: ${{ secrets.GDRIVE_SERVICE_ACCOUNT_JSON }}
        run: |
          pip install uv
          uv venv .venv
          uv pip install dvc[gdrive]
          
          # Pull data if DVC file exists
          if [ -f "data/raw/iris.csv.dvc" ]; then
            echo "Setting up DVC for Docker build..."
            echo "$GDRIVE_SERVICE_ACCOUNT_JSON" > /tmp/gdrive_service_account.json
            uv run dvc remote modify gdrive gdrive_use_service_account true
            uv run dvc remote modify gdrive gdrive_service_account_json_file_path /tmp/gdrive_service_account.json
            uv run dvc pull data/raw/iris.csv.dvc || echo "DVC pull failed, using cached data"
            rm -f /tmp/gdrive_service_account.json
          fi
      
      - name: Download model artifacts (if available)
        if: needs.model-training.outputs.model_trained == 'true'
        uses: actions/download-artifact@v4
        with:
          name: model-artifacts
          path: .
      
      - name: Commit updated production model
        if: needs.model-training.outputs.model_updated == 'true'
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          
          git add models/production_model/
          if ! git diff --staged --quiet; then
            git commit -m "Update production model from CI/CD [skip ci]"
            git push
          fi
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      
      - name: Log in to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ env.DOCKER_HUB_USERNAME }}
          password: ${{ env.DOCKER_HUB_TOKEN }}
      
      - name: Determine image tags
        id: tags
        run: |
          if [ "${{ github.event_name }}" = "pull_request" ]; then
            echo "IMAGE_TAG=pr-${{ github.event.number }}" >> $GITHUB_OUTPUT
          elif [ "${{ github.ref }}" = "refs/heads/master" ]; then
            echo "IMAGE_TAG=latest" >> $GITHUB_OUTPUT
          else
            echo "IMAGE_TAG=dev-${{ github.sha }}" >> $GITHUB_OUTPUT
          fi
          
          # Add model update suffix if model was updated
          if [ "${{ needs.model-training.outputs.model_updated }}" = "true" ]; then
            echo "MODEL_SUFFIX=-model-$(date +%Y%m%d-%H%M%S)" >> $GITHUB_OUTPUT
          else
            echo "MODEL_SUFFIX=" >> $GITHUB_OUTPUT
          fi
      
      - name: Build and push Docker images
        run: |
          # Build images with latest tag first
          export DOCKER_HUB_USERNAME=${{ env.DOCKER_HUB_USERNAME }}
          export IMAGE_TAG=latest
          docker compose build
          
          # Create additional tags
          FINAL_TAG=${{ steps.tags.outputs.IMAGE_TAG }}${{ steps.tags.outputs.MODEL_SUFFIX }}
          
          # Tag images with final tag
          docker tag $DOCKER_HUB_USERNAME/mlops-api:latest $DOCKER_HUB_USERNAME/mlops-api:$FINAL_TAG
          docker tag $DOCKER_HUB_USERNAME/mlops-ui:latest $DOCKER_HUB_USERNAME/mlops-ui:$FINAL_TAG
          docker tag $DOCKER_HUB_USERNAME/mlops-mlflow:latest $DOCKER_HUB_USERNAME/mlops-mlflow:$FINAL_TAG
          
          # Push images
          docker push $DOCKER_HUB_USERNAME/mlops-api:latest
          docker push $DOCKER_HUB_USERNAME/mlops-api:$FINAL_TAG
          docker push $DOCKER_HUB_USERNAME/mlops-ui:latest
          docker push $DOCKER_HUB_USERNAME/mlops-ui:$FINAL_TAG
          docker push $DOCKER_HUB_USERNAME/mlops-mlflow:latest
          docker push $DOCKER_HUB_USERNAME/mlops-mlflow:$FINAL_TAG
      
      - name: Test Docker images
        run: |
          echo "Testing API image..."
          docker run --rm -d --name test-api -p 8000:8000 ${{ env.DOCKER_HUB_USERNAME }}/mlops-api:latest
          
          sleep 15
          
          if curl -f http://localhost:8000/health; then
            echo "‚úÖ API health check passed!"
          else
            echo "‚ùå API health check failed"
            docker logs test-api
            docker stop test-api
            exit 1
          fi
          
          docker stop test-api

  # Stage 4: Pipeline Summary
  notify:
    runs-on: ubuntu-latest
    name: "Pipeline Summary"
    needs: [quality-check, model-training, build-and-deploy]
    if: always()
    
    steps:
      - name: Pipeline Summary
        run: |
          echo "=== MLOps CI/CD Pipeline Summary ==="
          echo "Quality Check: ${{ needs.quality-check.result }}"
          echo "Model Training: ${{ needs.model-training.result }}"
          echo "Build & Deploy: ${{ needs.build-and-deploy.result }}"
          
          if [ "${{ needs.build-and-deploy.result }}" = "success" ]; then
            if [ "${{ needs.model-training.outputs.model_updated }}" = "true" ]; then
              echo "üéâ Full pipeline with model update completed!"
              echo "‚úÖ Code quality checks passed"
              echo "‚úÖ Model retrained on updated data"
              echo "‚úÖ Model promoted to production"
              echo "‚úÖ Docker images built with updated model"
            else
              echo "‚úÖ Standard pipeline completed successfully!"
              echo "‚úÖ Code quality checks passed"
              echo "‚úÖ Docker images built and deployed"
            fi
          else
            echo "‚ùå Pipeline failed - check logs above"
          fi